{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "from settings import *\n",
    "import utils\n",
    "from callbacks import *\n",
    "from gan import GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU :)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "\n",
    "\ttry:\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "\t\tprint(\"Using GPU :)\")\n",
    "\n",
    "\texcept RuntimeError as e:\n",
    "\t\tprint(e)\n",
    "\n",
    "else:\n",
    "\tprint(\"Using CPU :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40603 files belonging to 1 classes.\n",
      "Dataset final size: 81206\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocessing.image_dataset_from_directory(\n",
    "\tDATA_DIR,\n",
    "\tlabel_mode = None,\n",
    "\tcolor_mode = \"rgb\",\n",
    "\tbatch_size = BATCH_SIZE,\n",
    "\timage_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "\tshuffle = True\n",
    ")\n",
    "\n",
    "dataset = dataset.map(utils.tf_norm_img)\n",
    "\n",
    "if FLIP_DATASET:\n",
    "\tflipped_dataset = dataset.map(tf.image.flip_left_right)\n",
    "\tdataset = dataset.concatenate(flipped_dataset)\n",
    "\tdataset = dataset.shuffle(BATCH_SIZE)\n",
    "\n",
    "print(\"Dataset final size:\", NB_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: (None, 512) -> (None, 512) | 2101248 parameters\n",
      "Generator: [(None, 1), (None, 512) * 6, (None, 128, 128, 1) * 11] -> (None, 128, 128, 3) | 17157085 parameters\n",
      "Discriminator: (None, 128, 128, 3) -> (None, 1) | 16438401 parameters\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.compile()\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run / Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAMPLES_DIR):\n",
    "\tos.makedirs(SAMPLES_DIR)\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "\tos.makedirs(MODELS_DIR)\n",
    "\n",
    "utils.reset_rand()\n",
    "initial_epoch = gan.load_weights(MODELS_DIR)\n",
    "\n",
    "if initial_epoch == 0:\n",
    "\tsamples_z = np.random.normal(0., 1., (OUTPUT_SHAPE[0] * OUTPUT_SHAPE[1], LATENT_DIM))\n",
    "\tsamples_noise = np.random.normal(0., 1., ((NB_BLOCKS * 2) - 1, OUTPUT_SHAPE[0] * OUTPUT_SHAPE[1], IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\tnp.save(os.path.join(OUTPUT_DIR, \"samples_z.npy\"), samples_z)\n",
    "\tnp.save(os.path.join(OUTPUT_DIR, \"samples_noise.npy\"), samples_noise)\n",
    "\n",
    "else:\n",
    "\tsamples_z = np.load(os.path.join(OUTPUT_DIR, \"samples_z.npy\"))\n",
    "\tsamples_noise = np.load(os.path.join(OUTPUT_DIR, \"samples_noise.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000\n",
      "    6/10152 [..............................] - ETA: 1:43:33 - Generator loss: 1.9327 - Discriminator loss: 0.7554 - Gradient penalty: 0.2701WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1291s vs `on_train_batch_end` time: 0.4309s). Check your callbacks.\n",
      "  311/10152 [..............................] - ETA: 1:00:23 - Generator loss: 1.7044 - Discriminator loss: 0.7587 - Gradient penalty: 0.2061"
     ]
    }
   ],
   "source": [
    "history = gan.fit(\n",
    "\tdataset,\n",
    "\tbatch_size = BATCH_SIZE,\n",
    "\tepochs = NB_EPOCHS,\n",
    "\tshuffle = True,\n",
    "\tinitial_epoch = initial_epoch,\n",
    "\tcallbacks = [\n",
    "\t\tUpdates(),\n",
    "\t\tSaveSamples(samples_z, samples_noise),\n",
    "\t\tSaveModels()\n",
    "\t]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49adeffe1bec12a08b3b1d5c77c627d0206a5fe107e4a03b623b1994225d96c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
