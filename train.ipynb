{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime faces generator (styleGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import utils\n",
    "from callbacks import UpdateVariables, SaveSamples, SaveModels\n",
    "from gan import GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU :)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "\n",
    "\ttry:\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "\t\tprint(\"Using GPU :)\")\n",
    "\n",
    "\texcept RuntimeError as e:\n",
    "\t\tprint(e)\n",
    "\n",
    "else:\n",
    "\tprint(\"Using CPU :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./datasets/dataset_128/\"\n",
    "IMAGE_SIZE = 128\n",
    "NB_CHANNELS = 3\n",
    "\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "SAMPLES_DIR = os.path.join(OUTPUT_DIR, \"images/\")\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models/\")\n",
    "OUTPUT_SHAPE = (7, 8)\n",
    "MARGIN = IMAGE_SIZE // 8\n",
    "SAVE_PER_EPOCH = 20\n",
    "\n",
    "LATENT_DIM = 512\n",
    "MAPPING_LAYERS = 8\n",
    "MIN_IMAGE_SIZE = 4\n",
    "MAX_FILTERS = 512\n",
    "MIN_FILTERS = 64\n",
    "KERNEL_SIZE = 3\n",
    "ALPHA = 0.2\n",
    "GAIN = 1.3\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "MAPPING_LR_RATIO = 0.01\n",
    "BETA_1 = 0.\n",
    "BETA_2 = 0.99\n",
    "EPSILON = 10e-8\n",
    "STYLE_MIX_PROBA = 0.5\n",
    "GRADIENT_PENALTY_COEF = 10.\n",
    "GRADIENT_PENALTY_INTERVAL = 4\n",
    "MA_HALF_LIFE = 10.\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NB_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30684 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocessing.image_dataset_from_directory(\n",
    "\tDATA_DIR,\n",
    "\tlabel_mode = None,\n",
    "\tcolor_mode = \"rgb\",\n",
    "\tbatch_size = BATCH_SIZE,\n",
    "\timage_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "\tshuffle = True\n",
    ")\n",
    "\n",
    "dataset = dataset.map(utils.norm_img)\n",
    "\n",
    "NB_DATA = len(os.listdir(DATA_DIR))\n",
    "NB_BATCHS = math.ceil(NB_DATA / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: (None, 512) -> (None, 512) | 2101248 parameters\n",
      "Generator: (None, 1) -> (None, 128, 128, 3) | 17157085 parameters\n",
      "Discriminator: (None, 128, 128, 3) -> (None, 1) | 16438401 parameters\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(\n",
    "\tlatent_dim = LATENT_DIM,\n",
    "\timage_size = IMAGE_SIZE,\n",
    "\tnb_channels = NB_CHANNELS,\n",
    "\tmapping_layers = MAPPING_LAYERS,\n",
    "\tmapping_lr_ratio = MAPPING_LR_RATIO,\n",
    "\tmin_image_size = MIN_IMAGE_SIZE,\n",
    "\tmax_filters = MAX_FILTERS,\n",
    "\tmin_filters = MIN_FILTERS,\n",
    "\tkernel_size = KERNEL_SIZE,\n",
    "\talpha = ALPHA,\n",
    "\tgain = GAIN,\n",
    "\tstyle_mix_proba = STYLE_MIX_PROBA,\n",
    "\tgp_coef = GRADIENT_PENALTY_COEF,\n",
    "\tgp_interval = GRADIENT_PENALTY_INTERVAL,\n",
    "\tma_beta = 0.5 ** (BATCH_SIZE / (MA_HALF_LIFE * 1000.)) if MA_HALF_LIFE > 0. else 0.,\n",
    "\tnb_batchs = NB_BATCHS\n",
    ")\n",
    "\n",
    "gan.compile(LEARNING_RATE, BETA_1, BETA_2, EPSILON)\n",
    "\n",
    "print(\"Mapping: {} -> {} | {} parameters\".format(gan.mapping.input_shape, gan.mapping.output_shape, gan.mapping.count_params()))\n",
    "print(\"Generator: {} -> {} | {} parameters\".format(gan.generator.input_shape[0], gan.generator.output_shape, gan.generator.count_params()))\n",
    "print(\"Discriminator: {} -> {} | {} parameters\".format(gan.discriminator.input_shape, gan.discriminator.output_shape, gan.discriminator.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run / Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAMPLES_DIR):\n",
    "\tos.makedirs(SAMPLES_DIR)\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "\tos.makedirs(MODELS_DIR)\n",
    "\n",
    "utils.reset_rand()\n",
    "initial_epoch = gan.load_weights(MODELS_DIR)\n",
    "\n",
    "if initial_epoch == 0:\n",
    "\tnb_blocks = int(math.log(IMAGE_SIZE, 2)) - int(math.log(MIN_IMAGE_SIZE, 2)) + 1\n",
    "\tsamples_z = np.random.normal(0., 1., (OUTPUT_SHAPE[0] * OUTPUT_SHAPE[1], LATENT_DIM))\n",
    "\tsamples_noise = np.random.normal(0., 1., ((nb_blocks * 2) - 1, OUTPUT_SHAPE[0] * OUTPUT_SHAPE[1], IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\tnp.save(os.path.join(OUTPUT_DIR, \"samples_z.npy\"), samples_z)\n",
    "\tnp.save(os.path.join(OUTPUT_DIR, \"samples_noise.npy\"), samples_noise)\n",
    "\n",
    "else:\n",
    "\tsamples_z = np.load(os.path.join(OUTPUT_DIR, \"samples_z.npy\"))\n",
    "\tsamples_noise = np.load(os.path.join(OUTPUT_DIR, \"samples_noise.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = gan.fit(\n",
    "\tdataset,\n",
    "\tbatch_size = BATCH_SIZE,\n",
    "\tepochs = NB_EPOCHS,\n",
    "\tshuffle = True,\n",
    "\tinitial_epoch = initial_epoch,\n",
    "\tcallbacks = [\n",
    "\t\tUpdateVariables(),\n",
    "\t\tSaveSamples(samples_z, samples_noise, BATCH_SIZE, MARGIN, OUTPUT_SHAPE, OUTPUT_DIR, SAMPLES_DIR, SAVE_PER_EPOCH, NB_BATCHS),\n",
    "\t\tSaveModels(MODELS_DIR)\n",
    "\t]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4e1cf01013ab48edb31e779a2d27747b973c68951b99f8c9f4467e823943775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
